{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from snac import SNAC\n",
    "from src.modules.fsq_vqvae import FSQVAE\n",
    "from src.modules.vqvae import VQVae\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "from train_tokenizer import VQVAEModule\n",
    "from src.dataset import Dataset\n",
    "\n",
    "model = SNAC.from_pretrained(\"hubertsiuzdak/snac_32khz\").eval().cuda()\n",
    "# audio = torch.randn(1, 1, 32000).cuda()  # placeholder for actual audio with shape (B, 1, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load audio from .wav file\n",
    "import torchaudio\n",
    "\n",
    "# Load and preprocess audio\n",
    "waveform, sample_rate = torchaudio.load(\"dataset/audio/_a-pB_5eRt0_7.wav\")\n",
    "# Convert to mono if stereo\n",
    "if waveform.shape[0] > 1:\n",
    "    waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "# Resample to 32kHz if needed\n",
    "if sample_rate != 24000:\n",
    "    resampler = torchaudio.transforms.Resample(sample_rate, 24000)\n",
    "    waveform = resampler(waveform)\n",
    "# Add batch dimension and move to GPU\n",
    "audio = waveform.unsqueeze(0).cuda()\n",
    "\n",
    "with torch.inference_mode():\n",
    "    codes = model.encode(audio)\n",
    "    audio_hat = model.decode(codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vqvae(model_path: Path) -> VQVae:\n",
    "    \"\"\"\n",
    "    Load and prepare a VQVAE model from a checkpoint file.\n",
    "    \n",
    "    Args:\n",
    "        model_path (Path): Path to the model checkpoint file\n",
    "        \n",
    "    Returns:\n",
    "        VQVae: Prepared VQVAE model loaded on CUDA and in eval mode\n",
    "    \"\"\"\n",
    "    pretrained = torch.load(model_path)\n",
    "    model_dir = model_path.parent.parent\n",
    "\n",
    "    if model_path.suffix == '.ckpt':\n",
    "        config_path = model_dir / 'wandb' / 'latest-run' / 'files' / 'config.yaml'\n",
    "    else:\n",
    "        config_path = model_path.parent / 'wandb' / 'latest-run' / 'files' / 'config.yaml'\n",
    "\n",
    "    config = yaml.safe_load(open(config_path, \"r\"))\n",
    "    feats_enabled = config['feats_enabled']['value']\n",
    "\n",
    "    print([feat for feat in sorted(feats_enabled) if feats_enabled[feat]['enabled']])\n",
    "    \n",
    "    if model_path.suffix == '.ckpt':\n",
    "        vqvae_module = VQVAEModule(vqvae_config=config[\"vqvae\"]['value'], losses_config=config[\"losses\"])\n",
    "        vqvae_module.load_state_dict(pretrained['state_dict'])\n",
    "        vqvae = vqvae_module.vqvae\n",
    "    else:\n",
    "        vqvae = VQVae(**config[\"vqvae\"]['value'])\n",
    "        vqvae.load_state_dict(pretrained)\n",
    "\n",
    "    vqvae.to(\"cuda\")\n",
    "    vqvae.eval()\n",
    "    \n",
    "    return vqvae, feats_enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c_eyes_lst', 'c_lip_lst', 'kp', 't', 'x_s']\n",
      "FSQ Config:\n",
      "Levels: [4, 4, 4, 4]\n",
      "Output Emb Width: 4\n",
      "Num Quantizers: 1\n",
      "Using FSQ\n"
     ]
    }
   ],
   "source": [
    "rest_fsq, rest_feats = load_vqvae(Path(\"outputs/rest_fsq_D4/checkpoints/checkpoint_epoch=1059.ckpt\"))\n",
    "rest_fsq: FSQVAE = rest_fsq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading precomputed statistics from dataset/stats_all.pkl\n",
      "Loaded feature-wise statistics successfully\n",
      "Loaded 4031 eval samples\n"
     ]
    }
   ],
   "source": [
    "ds = Dataset(\"dataset\", split=\"eval\", compute_stats=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_features(sample, feats_enabled, only_lips, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Prepare features from a sample by processing enabled features and concatenating them.\n",
    "    \n",
    "    Args:\n",
    "        sample (dict): Dictionary containing the sample data\n",
    "        feats_enabled (dict): Dictionary of enabled features and their metadata\n",
    "        device (str): Device to move tensors to (default: \"cuda\")\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (features tensor, dimensions dictionary)\n",
    "    \"\"\"\n",
    "    frames = sample['kp'].shape[0]\n",
    "    fps = sample['metadata']['output_fps']\n",
    "    seq_len = min(sample['kp'].shape[0], 300)\n",
    "\n",
    "    # Initialize an empty tensor list to collect features\n",
    "    feature_tensors = []\n",
    "    dims = {}\n",
    "\n",
    "    for feat, metadata in feats_enabled.items():\n",
    "        is_enabled = metadata['enabled']\n",
    "        if is_enabled:\n",
    "            print(f\"Using {feat}\")\n",
    "            if feat in [\"exp\", \"exp_velocity\"]:\n",
    "                if only_lips:\n",
    "                    feature = sample[feat][:seq_len, :, 15:, :].reshape(1, seq_len, -1)\n",
    "                else:\n",
    "                    feature = sample[feat][:seq_len, :, :15, :].reshape(1, seq_len, -1)\n",
    "            else:\n",
    "                feature = sample[feat][:seq_len, ...].reshape(1, seq_len, -1)\n",
    "            \n",
    "            dims[feat] = feature.shape[-1]\n",
    "            feature_tensors.append(feature)\n",
    "\n",
    "    # Concatenate all enabled features\n",
    "    if feature_tensors:\n",
    "        features = torch.concat(feature_tensors, dim=2)\n",
    "    else:\n",
    "        # Create an empty tensor if no features are enabled\n",
    "        features = torch.empty((1, seq_len, 0))\n",
    "\n",
    "    features = features.to(device)\n",
    "    print(\"dims: \", dims)\n",
    "    print(\"Total dims: \", features.shape[-1])\n",
    "    \n",
    "    return features, dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using c_eyes_lst\n",
      "Using c_lip_lst\n",
      "Using kp\n",
      "Using t\n",
      "Using x_s\n",
      "dims:  {'c_eyes_lst': 2, 'c_lip_lst': 1, 'kp': 63, 't': 3, 'x_s': 63}\n",
      "Total dims:  132\n"
     ]
    }
   ],
   "source": [
    "sample = ds[3364] \n",
    "\n",
    "pickle_path = sample['metadata']['pickle_path']\n",
    "vid_id = pickle_path.split(\"/\")[-1].split(\".\")[0]\n",
    "vid_path = f\"dataset/train/{vid_id}.mp4\"\n",
    "\n",
    "rest_features, rest_dims = prepare_features(sample, rest_feats, only_lips=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices shape: torch.Size([1, 145, 1])\n"
     ]
    }
   ],
   "source": [
    "indices = rest_fsq.encode(rest_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes = rest_fsq.quantizer.indices_to_codes(codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrest_fsq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcodes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/liveportrait-tokenizer/src/modules/fsq_vqvae.py:175\u001b[0m, in \u001b[0;36mFSQVAE.decode\u001b[0;34m(self, z)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, z: Tensor):\n\u001b[1;32m    170\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;124;03m    Decode from latent representation.\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;124;03m    For FSQ, we'd typically need to process the indices, but for simplicity\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;124;03m    we'll assume z contains the quantized representation directly.\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     N, T, D \u001b[38;5;241m=\u001b[39m z\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;66;03m# Reshape to expected format for decoder [N, C, T]\u001b[39;00m\n\u001b[1;32m    178\u001b[0m     z_reshaped \u001b[38;5;241m=\u001b[39m z\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 3)"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
