{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xabieririzar/liveportrait-tokenizer/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from snac import SNAC\n",
    "from src.modules.fsq_vqvae import FSQVAE\n",
    "from src.modules.vqvae import VQVae\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "from train_tokenizer import VQVAEModule\n",
    "from src.dataset import Dataset\n",
    "\n",
    "model = SNAC.from_pretrained(\"hubertsiuzdak/snac_32khz\").eval().cuda()\n",
    "# audio = torch.randn(1, 1, 32000).cuda()  # placeholder for actual audio with shape (B, 1, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load audio from .wav file\n",
    "import torchaudio\n",
    "\n",
    "# Load and preprocess audio\n",
    "waveform, sample_rate = torchaudio.load(\"dataset/audio/_a-pB_5eRt0_7.wav\")\n",
    "# Convert to mono if stereo\n",
    "if waveform.shape[0] > 1:\n",
    "    waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "# Resample to 32kHz if needed\n",
    "if sample_rate != 24000:\n",
    "    resampler = torchaudio.transforms.Resample(sample_rate, 24000)\n",
    "    waveform = resampler(waveform)\n",
    "# Add batch dimension and move to GPU\n",
    "audio = waveform.unsqueeze(0).cuda()\n",
    "\n",
    "with torch.inference_mode():\n",
    "    codes = model.encode(audio)\n",
    "    audio_hat = model.decode(codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vqvae(model_path: Path) -> VQVae:\n",
    "    \"\"\"\n",
    "    Load and prepare a VQVAE model from a checkpoint file.\n",
    "    \n",
    "    Args:\n",
    "        model_path (Path): Path to the model checkpoint file\n",
    "        \n",
    "    Returns:\n",
    "        VQVae: Prepared VQVAE model loaded on CUDA and in eval mode\n",
    "    \"\"\"\n",
    "    pretrained = torch.load(model_path)\n",
    "    model_dir = model_path.parent.parent\n",
    "\n",
    "    if model_path.suffix == '.ckpt':\n",
    "        config_path = model_dir / 'wandb' / 'latest-run' / 'files' / 'config.yaml'\n",
    "    else:\n",
    "        config_path = model_path.parent / 'wandb' / 'latest-run' / 'files' / 'config.yaml'\n",
    "\n",
    "    config = yaml.safe_load(open(config_path, \"r\"))\n",
    "    feats_enabled = config['feats_enabled']['value']\n",
    "\n",
    "    print([feat for feat in sorted(feats_enabled) if feats_enabled[feat]['enabled']])\n",
    "    \n",
    "    if model_path.suffix == '.ckpt':\n",
    "        vqvae_module = VQVAEModule(vqvae_config=config[\"vqvae\"]['value'], losses_config=config[\"losses\"])\n",
    "        vqvae_module.load_state_dict(pretrained['state_dict'])\n",
    "        vqvae = vqvae_module.vqvae\n",
    "    else:\n",
    "        vqvae = VQVae(**config[\"vqvae\"]['value'])\n",
    "        vqvae.load_state_dict(pretrained)\n",
    "\n",
    "    vqvae.to(\"cuda\")\n",
    "    vqvae.eval()\n",
    "    \n",
    "    return vqvae, feats_enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c_eyes_lst', 'c_lip_lst', 'kp', 't', 'x_s']\n",
      "FSQ Config:\n",
      "Levels: [4, 4, 4, 4]\n",
      "Output Emb Width: 4\n",
      "Num Quantizers: 1\n",
      "Using FSQ\n",
      "Codebook size: 256\n"
     ]
    }
   ],
   "source": [
    "rest_fsq, rest_feats = load_vqvae(Path(\"outputs/rest_fsq_D4/checkpoints/checkpoint_epoch=1059.ckpt\"))\n",
    "rest_fsq: FSQVAE = rest_fsq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading precomputed statistics from dataset/stats_all.pkl\n",
      "Loaded feature-wise statistics successfully\n",
      "Loaded 4090 eval samples\n"
     ]
    }
   ],
   "source": [
    "ds = Dataset(\"dataset\", split=\"eval\", compute_stats=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_features(sample, feats_enabled, only_lips, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Prepare features from a sample by processing enabled features and concatenating them.\n",
    "    \n",
    "    Args:\n",
    "        sample (dict): Dictionary containing the sample data\n",
    "        feats_enabled (dict): Dictionary of enabled features and their metadata\n",
    "        device (str): Device to move tensors to (default: \"cuda\")\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (features tensor, dimensions dictionary)\n",
    "    \"\"\"\n",
    "    frames = sample['kp'].shape[0]\n",
    "    fps = sample['metadata']['output_fps']\n",
    "    seq_len = min(sample['kp'].shape[0], 300)\n",
    "\n",
    "    # Initialize an empty tensor list to collect features\n",
    "    feature_tensors = []\n",
    "    dims = {}\n",
    "\n",
    "    for feat, metadata in feats_enabled.items():\n",
    "        is_enabled = metadata['enabled']\n",
    "        if is_enabled:\n",
    "            print(f\"Using {feat}\")\n",
    "            if feat in [\"exp\", \"exp_velocity\"]:\n",
    "                if only_lips:\n",
    "                    feature = sample[feat][:seq_len, :, 15:, :].reshape(1, seq_len, -1)\n",
    "                else:\n",
    "                    feature = sample[feat][:seq_len, :, :15, :].reshape(1, seq_len, -1)\n",
    "            else:\n",
    "                feature = sample[feat][:seq_len, ...].reshape(1, seq_len, -1)\n",
    "            \n",
    "            dims[feat] = feature.shape[-1]\n",
    "            feature_tensors.append(feature)\n",
    "\n",
    "    # Concatenate all enabled features\n",
    "    if feature_tensors:\n",
    "        features = torch.concat(feature_tensors, dim=2)\n",
    "    else:\n",
    "        # Create an empty tensor if no features are enabled\n",
    "        features = torch.empty((1, seq_len, 0))\n",
    "\n",
    "    features = features.to(device)\n",
    "    print(\"dims: \", dims)\n",
    "    print(\"Total dims: \", features.shape[-1])\n",
    "    \n",
    "    return features, dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using c_eyes_lst\n",
      "Using c_lip_lst\n",
      "Using kp\n",
      "Using t\n",
      "Using x_s\n",
      "dims:  {'c_eyes_lst': 2, 'c_lip_lst': 1, 'kp': 63, 't': 3, 'x_s': 63}\n",
      "Total dims:  132\n"
     ]
    }
   ],
   "source": [
    "sample = ds[3364] \n",
    "\n",
    "pickle_path = sample['metadata']['pickle_path']\n",
    "vid_id = pickle_path.split(\"/\")[-1].split(\".\")[0]\n",
    "vid_path = f\"dataset/train/{vid_id}.mp4\"\n",
    "\n",
    "rest_features, rest_dims = prepare_features(sample, rest_feats, only_lips=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pickle_path': 'dataset/pickles/VTKRMo7HYkY_0.pkl',\n",
       " 'n_frames': 100,\n",
       " 'output_fps': 23.98}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['metadata']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices shape: torch.Size([1, 98, 1])\n"
     ]
    }
   ],
   "source": [
    "indices = rest_fsq.encode(rest_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 10],\n",
       "         [ 91],\n",
       "         [182],\n",
       "         [119],\n",
       "         [162],\n",
       "         [ 81],\n",
       "         [ 15],\n",
       "         [255],\n",
       "         [241],\n",
       "         [243],\n",
       "         [227],\n",
       "         [171],\n",
       "         [207],\n",
       "         [207],\n",
       "         [250],\n",
       "         [242],\n",
       "         [163],\n",
       "         [231],\n",
       "         [158],\n",
       "         [ 95],\n",
       "         [251],\n",
       "         [242],\n",
       "         [167],\n",
       "         [163],\n",
       "         [ 86],\n",
       "         [ 79],\n",
       "         [155],\n",
       "         [183],\n",
       "         [167],\n",
       "         [146],\n",
       "         [ 86],\n",
       "         [ 91],\n",
       "         [183],\n",
       "         [150],\n",
       "         [211],\n",
       "         [178],\n",
       "         [ 27],\n",
       "         [155],\n",
       "         [182],\n",
       "         [227],\n",
       "         [162],\n",
       "         [ 98],\n",
       "         [ 95],\n",
       "         [187],\n",
       "         [166],\n",
       "         [215],\n",
       "         [ 97],\n",
       "         [ 38],\n",
       "         [ 75],\n",
       "         [ 91],\n",
       "         [115],\n",
       "         [167],\n",
       "         [101],\n",
       "         [ 23],\n",
       "         [ 75],\n",
       "         [103],\n",
       "         [119],\n",
       "         [150],\n",
       "         [161],\n",
       "         [102],\n",
       "         [ 87],\n",
       "         [103],\n",
       "         [215],\n",
       "         [178],\n",
       "         [ 37],\n",
       "         [  7],\n",
       "         [167],\n",
       "         [182],\n",
       "         [ 98],\n",
       "         [ 33],\n",
       "         [ 11],\n",
       "         [ 87],\n",
       "         [119],\n",
       "         [167],\n",
       "         [ 97],\n",
       "         [ 34],\n",
       "         [ 87],\n",
       "         [ 99],\n",
       "         [118],\n",
       "         [162],\n",
       "         [ 97],\n",
       "         [ 18],\n",
       "         [ 23],\n",
       "         [103],\n",
       "         [182],\n",
       "         [162],\n",
       "         [ 97],\n",
       "         [ 18],\n",
       "         [ 23],\n",
       "         [119],\n",
       "         [118],\n",
       "         [162],\n",
       "         [161],\n",
       "         [ 23],\n",
       "         [ 87],\n",
       "         [119],\n",
       "         [166],\n",
       "         [166]]], device='cuda:0', dtype=torch.int32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 98, 1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rest_fsq.quantizer.indices_to_codes(indices).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
