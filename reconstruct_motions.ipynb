{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xabieririzar/liveportrait-tokenizer/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import os\n",
    "from random import seed\n",
    "import pickle\n",
    "import yaml\n",
    "import imageio\n",
    "\n",
    "from src.modules.vqvae import VQVae\n",
    "from src.modules.fsq_vqvae import FSQVAE\n",
    "from src.modules.res_vqvae import ResVQVae\n",
    "from train_tokenizer import VQVAEModule\n",
    "from src.dataset import Dataset\n",
    "from IPython.display import Image, display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from vector_quantize_pytorch import FSQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_dir = Path(\"dataset/pickles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vqvae(model_path: Path) -> VQVae:\n",
    "    \"\"\"\n",
    "    Load and prepare a VQVAE model from a checkpoint file.\n",
    "    \n",
    "    Args:\n",
    "        model_path (Path): Path to the model checkpoint file\n",
    "        \n",
    "    Returns:\n",
    "        VQVae: Prepared VQVAE model loaded on CUDA and in eval mode\n",
    "    \"\"\"\n",
    "    pretrained = torch.load(model_path)\n",
    "    model_dir = model_path.parent.parent\n",
    "\n",
    "    if model_path.suffix == '.ckpt':\n",
    "        config_path = model_dir / 'wandb' / 'latest-run' / 'files' / 'config.yaml'\n",
    "    else:\n",
    "        config_path = model_path.parent / 'wandb' / 'latest-run' / 'files' / 'config.yaml'\n",
    "\n",
    "    config = yaml.safe_load(open(config_path, \"r\"))\n",
    "    feats_enabled = config['feats_enabled']['value']\n",
    "\n",
    "    print([feat for feat in sorted(feats_enabled) if feats_enabled[feat]['enabled']])\n",
    "    \n",
    "    if model_path.suffix == '.ckpt':\n",
    "        vqvae_module = VQVAEModule(vqvae_config=config[\"vqvae\"]['value'], losses_config=config[\"losses\"])\n",
    "        vqvae_module.load_state_dict(pretrained['state_dict'])\n",
    "        vqvae = vqvae_module.vqvae\n",
    "    else:\n",
    "        vqvae = VQVae(**config[\"vqvae\"]['value'])\n",
    "        vqvae.load_state_dict(pretrained)\n",
    "\n",
    "    vqvae.to(\"cuda\")\n",
    "    vqvae.eval()\n",
    "    \n",
    "    return vqvae, feats_enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c_eyes_lst', 'c_lip_lst', 'kp', 't', 'x_s']\n",
      "FSQ Config:\n",
      "Levels: [4, 4, 4, 4]\n",
      "Output Emb Width: 4\n",
      "Num Quantizers: 1\n",
      "Using FSQ\n"
     ]
    }
   ],
   "source": [
    "rest_fsq, rest_feats = load_vqvae(Path(\"outputs/rest_fsq_D4/checkpoints/checkpoint_epoch=1059.ckpt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['R', 'scale']\n",
      "FSQ Config:\n",
      "Levels: [4, 4, 4, 4, 4, 4, 4, 4]\n",
      "Output Emb Width: 8\n",
      "Num Quantizers: 1\n",
      "Using FSQ\n"
     ]
    }
   ],
   "source": [
    "rot_scale_fsq, rot_scale_feats = load_vqvae(Path(\"outputs/20250524_142315-vae-fsq-rot_scale-D8-L4-lr1e-4-bs32-e2000/checkpoints/checkpoint_epoch=309.ckpt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['exp']\n",
      "FSQ Config:\n",
      "Levels: [4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "Output Emb Width: 10\n",
      "Num Quantizers: 1\n",
      "Using FSQ\n"
     ]
    }
   ],
   "source": [
    "exp_fsq, exp_feats = load_vqvae(Path(\"outputs/20250524_170400-vae-fsq-only_exp_rest-D10-L4-lr1e-4-bs32-e2000/checkpoints/checkpoint_epoch=909.ckpt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['exp', 'exp_velocity']\n",
      "FSQ Config:\n",
      "Levels: [4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "Output Emb Width: 10\n",
      "Num Quantizers: 1\n",
      "Using FSQ\n"
     ]
    }
   ],
   "source": [
    "lip_fsq, lip_feats = load_vqvae(Path(\"outputs/lip_fsq/checkpoints/checkpoint_epoch=239.ckpt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing statistics for entire dataset using 8 threads...\n",
      "Computing statistics for 20447 samples using 8 threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 20447/20447 [25:34<00:00, 13.32it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacked 3826634 values for feature kp\n",
      "Stacked 3826634 values for feature exp\n",
      "Stacked 3826634 values for feature x_s\n",
      "Stacked 3826634 values for feature t\n",
      "Stacked 3826634 values for feature R\n",
      "Stacked 3826634 values for feature scale\n",
      "Stacked 3826634 values for feature c_eyes_lst\n",
      "Stacked 3826634 values for feature c_lip_lst\n",
      "Stacked 3826634 values for feature kp_velocity\n",
      "Stacked 3826634 values for feature exp_velocity\n",
      "Stacked 3826634 values for feature kp_acceleration\n",
      "Stacked 3826634 values for feature exp_acceleration\n",
      "Stacked 3826634 values for feature R_velocity\n",
      "Stacked 3826634 values for feature scale_velocity\n",
      "Computed feature-wise statistics successfully\n",
      "Statistics saved to dataset/stats_all.pkl\n",
      "Loaded 4090 eval samples\n"
     ]
    }
   ],
   "source": [
    "ds = Dataset(\"dataset\", split=\"eval\", compute_stats=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_id = \"droRkoEh8iE_18\"\n",
    "# # test_id = \"WQvT1_tQDhg_22\"\n",
    "# find_id = f\"dataset/pickles/{test_id}.pkl\"\n",
    "\n",
    "# for i, item in enumerate(ds):\n",
    "#     if item['metadata']['pickle_path'] == find_id:\n",
    "#         print(seed, i)\n",
    "#         break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare ds item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1746 guy video\n",
    "# 3364 girl video\n",
    "\n",
    "sample = ds[3364] \n",
    "\n",
    "pickle_path = sample['metadata']['pickle_path']\n",
    "vid_id = pickle_path.split(\"/\")[-1].split(\".\")[0]\n",
    "vid_path = f\"dataset/train/{vid_id}.mp4\"\n",
    "\n",
    "# Read the first frame from the video and display it\n",
    "frame = imageio.get_reader(vid_path).get_data(0)\n",
    "plt.imshow(frame)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_features(sample, feats_enabled, only_lips, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Prepare features from a sample by processing enabled features and concatenating them.\n",
    "    \n",
    "    Args:\n",
    "        sample (dict): Dictionary containing the sample data\n",
    "        feats_enabled (dict): Dictionary of enabled features and their metadata\n",
    "        device (str): Device to move tensors to (default: \"cuda\")\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (features tensor, dimensions dictionary)\n",
    "    \"\"\"\n",
    "    frames = sample['kp'].shape[0]\n",
    "    fps = sample['metadata']['output_fps']\n",
    "    seq_len = min(sample['kp'].shape[0], 300)\n",
    "\n",
    "    # Initialize an empty tensor list to collect features\n",
    "    feature_tensors = []\n",
    "    dims = {}\n",
    "\n",
    "    for feat, metadata in feats_enabled.items():\n",
    "        is_enabled = metadata['enabled']\n",
    "        if is_enabled:\n",
    "            print(f\"Using {feat}\")\n",
    "            if feat in [\"exp\", \"exp_velocity\"]:\n",
    "                if only_lips:\n",
    "                    feature = sample[feat][:seq_len, :, 15:, :].reshape(1, seq_len, -1)\n",
    "                else:\n",
    "                    feature = sample[feat][:seq_len, :, :15, :].reshape(1, seq_len, -1)\n",
    "            else:\n",
    "                feature = sample[feat][:seq_len, ...].reshape(1, seq_len, -1)\n",
    "            \n",
    "            dims[feat] = feature.shape[-1]\n",
    "            feature_tensors.append(feature)\n",
    "\n",
    "    # Concatenate all enabled features\n",
    "    if feature_tensors:\n",
    "        features = torch.concat(feature_tensors, dim=2)\n",
    "    else:\n",
    "        # Create an empty tensor if no features are enabled\n",
    "        features = torch.empty((1, seq_len, 0))\n",
    "\n",
    "    features = features.to(device)\n",
    "    print(\"dims: \", dims)\n",
    "    print(\"Total dims: \", features.shape[-1])\n",
    "    \n",
    "    return features, dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_features, exp_dims = prepare_features(sample, exp_feats, only_lips=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rest_features, rest_dims = prepare_features(sample, rest_feats, only_lips=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_scale_features, r_scale_dims = prepare_features(sample, rot_scale_feats, only_lips=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats_lip, lip_dims = prepare_features(sample, lip_feats, only_lips=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load stats\n",
    "stats = pickle.load(open(\"dataset/stats_all.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send stats to GPU\n",
    "for key in stats['mean']:\n",
    "    stats['mean'][key] = stats['mean'][key].to(\"cuda\")\n",
    "    stats['std'][key] = stats['std'][key].to(\"cuda\")\n",
    "std = stats['std']\n",
    "mean = stats['mean']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstruct features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    reconstr_rest = rest_fsq(rest_features)\n",
    "\n",
    "    reconstr_r_scale = rot_scale_fsq(r_scale_features)\n",
    "\n",
    "    reconstr_exp_rest = exp_fsq(exp_features)\n",
    "    reconstr_lip = lip_fsq(feats_lip)\n",
    "\n",
    "    # Remove velocity components\n",
    "    reconstr_exp = torch.cat([reconstr_exp_rest[..., :45], reconstr_lip[..., :18]], dim=-1)\n",
    "    exp_dims['exp'] = 63\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test encoding features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "#     encodings = rest_fsq.encode(rest_features)\n",
    "#     result = rest_fsq.decode(encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_reconstruction(dims, reconstr, exp_lips, std, mean):\n",
    "    \"\"\"\n",
    "    Process reconstruction tensor by filtering out velocity dimensions and applying normalization.\n",
    "    \n",
    "    Args:\n",
    "        dims (dict): Dictionary containing feature dimensions\n",
    "        reconstr (torch.Tensor): Reconstruction tensor\n",
    "        std (dict): Dictionary of standard deviations for normalization\n",
    "        mean (dict): Dictionary of means for normalization\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: Processed reconstruction tensor\n",
    "    \"\"\"\n",
    "    # Filter out velocity dimensions and add lip dimensions to exp\n",
    "    filtered_dims = {k: v for k, v in dims.items() if k not in ['kp_velocity', 'kp_acceleration', 'exp_velocity']}\n",
    "\n",
    "    total_dims = sum(filtered_dims.values())\n",
    "\n",
    "    # Initialize output tensor\n",
    "    new_reconstr = torch.zeros((*reconstr.shape[:-1], total_dims))\n",
    "\n",
    "    cur_ind = 0\n",
    "    reconstr_ind = 0\n",
    "    for feat, indices in dims.items():\n",
    "        if 'velocity' in feat:\n",
    "            # Skip velocity in new_reconstr but still increment reconstr_ind\n",
    "            reconstr_ind += indices\n",
    "            continue\n",
    "            \n",
    "        print(f\"{feat} {cur_ind} : {cur_ind + indices}\")\n",
    "        new_reconstr[..., cur_ind:cur_ind + indices] = reconstr[..., reconstr_ind:reconstr_ind + indices] * std[feat] + mean[feat]\n",
    "            \n",
    "        cur_ind += indices\n",
    "        reconstr_ind += indices\n",
    "\n",
    "    return new_reconstr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rest_new_reconstr = process_reconstruction(rest_dims, reconstr_rest, False, std, mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_scale_new_reconstr = process_reconstruction(r_scale_dims, reconstr_r_scale, False, std, mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_new_reconstr = process_reconstruction(exp_dims, reconstr_exp, False, std, mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_reconstr = torch.zeros((*rest_new_reconstr.shape[:-1], 205))\n",
    "new_reconstr[..., :9] = r_scale_new_reconstr[..., :9]\n",
    "new_reconstr[..., 9:12] = rest_new_reconstr[..., :3]\n",
    "new_reconstr[..., 12:75] = exp_new_reconstr\n",
    "new_reconstr[..., 75:138] = rest_new_reconstr[..., 3:66]\n",
    "new_reconstr[..., 138:139] = rest_new_reconstr[..., 75:76]\n",
    "new_reconstr[..., 138:139] = r_scale_new_reconstr[..., 9:10]\n",
    "new_reconstr[..., 139:142] = rest_new_reconstr[..., 66:69]\n",
    "new_reconstr[..., 142:205] = rest_new_reconstr[..., 69:132]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dims = {\n",
    "    'R': 9,\n",
    "    'c_eyes_lst': 2,\n",
    "    'c_lip_lst': 1,\n",
    "    'exp': 63,\n",
    "    'kp': 63,\n",
    "    'scale': 1,\n",
    "    't': 3,\n",
    "    'x_s': 63\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zeros = torch.zeros(1, new_reconstr.shape[1], new_reconstr.shape[-1] + 18)\n",
    "# # full_exp = sample['exp'][:95].reshape(1, 95, -1).to('cuda')\n",
    "\n",
    "# cur_ind = 0\n",
    "# reconstr_ind = 0\n",
    "\n",
    "# for feat, indices in dims.items():\n",
    "#     print(f\"{feat}: {cur_ind}: {cur_ind + indices}\")\n",
    "#     if feat == 'exp':\n",
    "#         zeros[..., cur_ind: cur_ind + indices] = new_reconstr[..., reconstr_ind: reconstr_ind + indices] \n",
    "#         # zeros[..., cur_ind: cur_ind + indices] = full_exp[..., reconstr_ind: reconstr_ind + indices] \n",
    "#         zeros[..., cur_ind + indices: cur_ind + indices + 18] = reconst_lip * std[feat][:, 45:] + mean[feat][:, 45:]\n",
    "#         cur_ind += indices + 18\n",
    "        \n",
    "#         reconstr_ind += indices\n",
    "#     else:\n",
    "#         zeros[..., cur_ind: cur_ind + indices] = new_reconstr[..., reconstr_ind: reconstr_ind + indices]\n",
    "#         cur_ind += indices\n",
    "#         reconstr_ind += indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_feats = sample['exp'][:300].reshape(1, 300, -1).to('cuda')\n",
    "# zeros = torch.zeros_like(full_feats, device='cuda')\n",
    "# zeros[..., :45] = new_reconstr\n",
    "# zeros[..., 45:] = full_feats[..., 45:] * std['exp'][:, 45:] + mean['exp'][:, 45:]\n",
    "\n",
    "# new_reconstr = zeros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate reconstructed pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_rest_feats = {}\n",
    "\n",
    "for key, value in rest_feats.items():\n",
    "    new_rest_feats[key] = value\n",
    "    new_rest_feats[key]['enabled'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare output\n",
    "def repackage_output(original, reconstr, dataset, dims):\n",
    "    # Get normalized tensors from reconstruction\n",
    "    reconstr = reconstr.to('cpu').squeeze(0)\n",
    "    frames = reconstr.shape[0]\n",
    "    print(\"Frames: \", frames)\n",
    "\n",
    "    start = 0\n",
    "    end = 0\n",
    "\n",
    "    main_feats = {\n",
    "        \"kp\": None,\n",
    "        \"exp\": None,\n",
    "        \"x_s\": None,\n",
    "        \"t\": None,\n",
    "        \"R\": None,\n",
    "        \"scale\": None,\n",
    "        \"c_eyes_lst\": None,\n",
    "        \"c_lip_lst\": None\n",
    "    }\n",
    "\n",
    "    for feat, metadata in new_rest_feats.items():\n",
    "        is_enabled = metadata['enabled']\n",
    "        feat_shape = metadata['shape']\n",
    "\n",
    "        if is_enabled and feat in main_feats:\n",
    "            end += dims[feat]\n",
    "            print(f\"{feat}: {start}:{end}\")\n",
    "            if feat == \"exp\":\n",
    "                rec_feat = reconstr[:, start:end].reshape(-1, *feat_shape)\n",
    "            else:\n",
    "                rec_feat = reconstr[:, start:end].reshape(-1, *feat_shape)\n",
    "            main_feats[feat] = rec_feat\n",
    "            start += dims[feat]\n",
    "    \n",
    "    # Denormalize original features too (since they come from __getitem__)\n",
    "    orig_kps = dataset.denormalize_features(original['kp'], \"kp\")\n",
    "    orig_exp = dataset.denormalize_features(original['exp'], \"exp\")\n",
    "    orig_x_s = dataset.denormalize_features(original['x_s'], \"x_s\")\n",
    "    orig_t = dataset.denormalize_features(original['t'], \"t\")\n",
    "    orig_R = dataset.denormalize_features(original['R'], \"R\")\n",
    "    orig_scale = dataset.denormalize_features(original['scale'], \"scale\").squeeze(-1)\n",
    "    orig_c_eyes_lst = dataset.denormalize_features(original['c_eyes_lst'], \"c_eyes_lst\")\n",
    "    orig_c_lip_lst = dataset.denormalize_features(original['c_lip_lst'], \"c_lip_lst\")\n",
    "\n",
    "    # Print which features are enabled and will be used\n",
    "    print(\"Enabled features:\")\n",
    "    for feat, metadata in new_rest_feats.items():\n",
    "        if metadata['enabled'] and 'velocity' not in feat and 'acceleration' not in feat:\n",
    "            print(f\"- {feat}\")\n",
    "\n",
    "    n_frames = min(original['metadata']['n_frames'], frames)\n",
    "\n",
    "    output = {\n",
    "        \"n_frames\": n_frames,\n",
    "        \"output_fps\": original['metadata']['output_fps'],\n",
    "        \"motion\": [\n",
    "            {\n",
    "                \"kp\": main_feats['kp'][i].cpu().numpy() if new_rest_feats['kp']['enabled'] else orig_kps[i].cpu().numpy(),\n",
    "                \"exp\": main_feats['exp'][i].cpu().numpy() if new_rest_feats['exp']['enabled'] else orig_exp[i].cpu().numpy(),\n",
    "                \"x_s\": main_feats['x_s'][i].cpu().numpy() if new_rest_feats['x_s']['enabled'] else orig_x_s[i].cpu().numpy(),\n",
    "                \"t\": main_feats['t'][i].cpu().numpy() if new_rest_feats['t']['enabled'] else orig_t[i].cpu().numpy(),\n",
    "                \"R\": main_feats['R'][i].cpu().numpy() if new_rest_feats['R']['enabled'] else orig_R[i].cpu().numpy(),\n",
    "                \"scale\": main_feats['scale'][i].cpu().numpy() if new_rest_feats['scale']['enabled'] else orig_scale[i].cpu().numpy(),\n",
    "            } for i in range(frames)\n",
    "        ],\n",
    "        \"c_eyes_lst\": [main_feats['c_eyes_lst'][i].cpu().numpy() if new_rest_feats['c_eyes_lst']['enabled'] else orig_c_eyes_lst[i].cpu().numpy() for i in range(frames)],\n",
    "        \"c_lip_lst\": [main_feats['c_lip_lst'][i].cpu().numpy() if new_rest_feats['c_lip_lst']['enabled'] else orig_c_lip_lst[i].cpu().numpy() for i in range(frames)],\n",
    "    }\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = repackage_output(sample, new_reconstr, ds, dims=new_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Replace reconstructed features with original ones for a specific range\n",
    "# feat = 'exp'\n",
    "\n",
    "# feat_range = (0, 15)\n",
    "\n",
    "# for i in range(output['n_frames']):\n",
    "#     output['motion'][i][feat][:, feat_range[0]:feat_range[1]] = sample[feat][i][:, feat_range[0]:feat_range[1]]\n",
    "\n",
    "# denormalized_sample = ds.denormalize_features(sample[feat], feat)\n",
    "\n",
    "# for i in range(output['n_frames']):\n",
    "#     output['motion'][i][feat] = denormalized_sample[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_id = Path(pickle_path).stem\n",
    "new_path = pickle_dir / f\"{video_id}_reconstructed.pkl\"\n",
    "print(new_path)\n",
    "with open(new_path, \"wb\") as f:\n",
    "    pickle.dump(output, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "\n",
    "vid = imageio.get_reader(f\"dataset/train/{video_id}.mp4\")\n",
    "frame = vid.get_data(0)\n",
    "\n",
    "# save frame\n",
    "imageio.imwrite(\"assets/examples/source/reconstructed.png\", frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python inference.py -d {new_path} -s assets/examples/source/reconstructed.png"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
