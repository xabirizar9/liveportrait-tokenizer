{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xabieririzar/liveportrait-tokenizer/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "from snac import SNAC\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from src import MotionDataset\n",
    "from src import TokenizerModule\n",
    "from src.full_dataset import SNACMotionTextDataset\n",
    "\n",
    "from IPython.display import Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_tokenizer = SNAC.from_pretrained(\"hubertsiuzdak/snac_24khz\").eval().cuda()\n",
    "motion_tokenizer = TokenizerModule.from_pretrained(\"InternalCan/tokenizer_module\")\n",
    "text_tokenizer = AutoTokenizer.from_pretrained(\"canopylabs/orpheus-3b-0.1-pretrained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_ds = SNACMotionTextDataset(\"dataset\", split=\"val\", val_split=0.1, seed=2, compute_stats=False, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = full_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = full_ds[134]\n",
    "\n",
    "audio = sample['audio']\n",
    "motion = full_ds.resample_item(sample['motion'], 24)\n",
    "\n",
    "with torch.no_grad():\n",
    "    motion_features = motion_tokenizer.sample_to_features(motion)\n",
    "    motion_tokens = motion_tokenizer.features_to_codes(motion_features)\n",
    "    audio_tokens = audio_tokenizer.encode(audio.unsqueeze(0).to('cuda'))\n",
    "    text_tokens = text_tokenizer.encode(sample['text'], add_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the functions\n",
    "frame_tokens = merge_tokens(audio_tokens, motion_tokens)\n",
    "\n",
    "example = {\n",
    "    \"speech_motion_tokens\": frame_tokens,\n",
    "    \"text_tokens\": text_tokens\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokeniser_length = 128256\n",
    "start_of_text = 128000\n",
    "end_of_text = 128009\n",
    "\n",
    "start_of_speech = tokeniser_length + 1\n",
    "end_of_speech = tokeniser_length + 2\n",
    "\n",
    "start_of_human = tokeniser_length + 3\n",
    "end_of_human = tokeniser_length + 4\n",
    "\n",
    "start_of_ai = tokeniser_length + 5\n",
    "end_of_ai =  tokeniser_length + 6\n",
    "pad_token = tokeniser_length + 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input_ids(example):\n",
    "    text_tokens = example['text_tokens']\n",
    "    speech_motion_tokens = example['speech_motion_tokens'].flatten().tolist()\n",
    "\n",
    "    text_tokens.append(end_of_text)\n",
    "\n",
    "    input_ids = (\n",
    "        [start_of_human]\n",
    "        + text_tokens\n",
    "        + [end_of_human]\n",
    "        + [start_of_ai]\n",
    "        + [start_of_speech]\n",
    "        + speech_motion_tokens\n",
    "        + [end_of_speech]\n",
    "        + [end_of_ai]\n",
    "    )\n",
    "\n",
    "    output = {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"labels\": input_ids,\n",
    "        \"attention_mask\": [1] * len(input_ids)\n",
    "    }\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "def parse_input_ids(input_ids):\n",
    "    \"\"\"\n",
    "    Parse input_ids back into text tokens and speech_motion tokens.\n",
    "    \n",
    "    Args:\n",
    "        input_ids: List of token IDs created by create_input_ids\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing 'text_tokens' and 'speech_motion_tokens'\n",
    "    \"\"\"\n",
    "    # Find positions of special tokens\n",
    "    try:\n",
    "        start_human_idx = input_ids.index(start_of_human)\n",
    "        end_human_idx = input_ids.index(end_of_human)\n",
    "        start_ai_idx = input_ids.index(start_of_ai)\n",
    "        start_speech_idx = input_ids.index(start_of_speech)\n",
    "        end_speech_idx = input_ids.index(end_of_speech)\n",
    "        end_ai_idx = input_ids.index(end_of_ai)\n",
    "    except ValueError as e:\n",
    "        raise ValueError(f\"Missing expected special tokens in input_ids: {e}\")\n",
    "    \n",
    "    # Extract text tokens (between start_of_human and end_of_human, excluding end_of_text)\n",
    "    text_tokens = input_ids[start_human_idx + 1:end_human_idx]\n",
    "    if text_tokens and text_tokens[-1] == end_of_text:\n",
    "        text_tokens = text_tokens[:-1]  # Remove end_of_text token\n",
    "    \n",
    "    # Extract speech motion tokens (between start_of_speech and end_of_speech)\n",
    "    speech_motion_tokens = input_ids[start_speech_idx + 1:end_speech_idx]\n",
    "    \n",
    "    return {\n",
    "        'text_tokens': text_tokens,\n",
    "        'speech_motion_tokens': speech_motion_tokens\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'example' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m create_input_ids(\u001b[43mexample\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'example' is not defined"
     ]
    }
   ],
   "source": [
    "input_ids = create_input_ids(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19988/19988 [01:22<00:00, 241.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created dataset with 19988 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 19988/19988 [00:00<00:00, 69379.89 examples/s] \n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load all .pt files from the tokens directory\n",
    "token_dir = Path(\"dataset/tokens\")\n",
    "pt_files = list(token_dir.glob(\"*.pt\"))\n",
    "\n",
    "# Collect all input_ids from the .pt files\n",
    "all_input_ids = []\n",
    "all_metadata = []\n",
    "\n",
    "for pt_file in tqdm(pt_files):\n",
    "    try:\n",
    "        data = torch.load(pt_file)\n",
    "        if 'input_ids' in data:\n",
    "            all_input_ids.append(data['input_ids'])\n",
    "            # Store metadata if available\n",
    "            metadata = data.get('metadata', {})\n",
    "            metadata['file_path'] = str(pt_file)\n",
    "            all_metadata.append(metadata)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {pt_file}: {e}\")\n",
    "\n",
    "# Create HuggingFace dataset\n",
    "dataset_dict = {\n",
    "    'input_ids': all_input_ids,\n",
    "    'metadata': all_metadata\n",
    "}\n",
    "\n",
    "hf_dataset = Dataset.from_dict(dataset_dict)\n",
    "print(f\"Created dataset with {len(hf_dataset)} samples\")\n",
    "\n",
    "# Save dataset locally\n",
    "hf_dataset.save_to_disk(\"dataset/hf_dataset\")\n",
    "\n",
    "# Push to HuggingFace Hub (uncomment and set your repo name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 20/20 [00:00<00:00, 25.87ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:03<00:00,  3.11s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/InternalCan/snac_motion_text_dataset/commit/3aa604478f1daa2165c0e15fc6d6612831b0acfd', commit_message='Upload dataset', commit_description='', oid='3aa604478f1daa2165c0e15fc6d6612831b0acfd', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/InternalCan/snac_motion_text_dataset', endpoint='https://huggingface.co', repo_type='dataset', repo_id='InternalCan/snac_motion_text_dataset'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_dataset.push_to_hub(\"InternalCan/snac_motion_text_dataset\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
